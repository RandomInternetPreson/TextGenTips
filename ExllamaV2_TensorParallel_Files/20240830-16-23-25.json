{
    "internal": [
        [
            "<|BEGIN-VISIBLE-CHAT|>",
            "How can I help you today?"
        ],
        [
            "Hello, how are you functioning today?",
            "I'm functioning well, thank you for asking! I'm here and ready to assist you. How about you? How's your day going?"
        ],
        [
            "I'm doing well too.  There is this quantization for LLMs called exllamaV2, recently it has implemented tensor parallelism, so folks running models on multi-gpu systems can inference faster....much faster in fact.  ExllamaV2 is a loader in a framework called textgeneration webui.  This allows different loaders such as ExllamaV2.\n\nSo I have exllamaV2 working with textgen using the exllamav2 loader, but i want to also get it working with the exllamav2_HF loader, this is a huggingface specific implementation of exllamav2.  I'll give you the code for both:\n\nCurrently working exllamav2 code:\n #!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Thu Aug 29 18:06:29 2024\n\n@author: myself\n\"\"\"\n\nimport traceback\nfrom pathlib import Path\n\nimport torch\nfrom exllamav2 import (\n      ExLlamaV2,\n    ExLlamaV2Config,\n    ExLlamaV2Cache,\n    ExLlamaV2Cache_8bit,\n    ExLlamaV2Cache_Q4,\n    ExLlamaV2Cache_Q6,\n    ExLlamaV2Cache_Q8,\n    ExLlamaV2Cache_TP,\n    ExLlamaV2Tokenizer,\n    model_init,\n)\nfrom exllamav2.generator import ExLlamaV2Sampler, ExLlamaV2StreamingGenerator\n\nfrom modules import shared\nfrom modules.logging_colors import logger\nfrom modules.text_generation import get_max_prompt_length\n\ntry:\n    import flash_attn\nexcept ModuleNotFoundError:\n    logger.warning(\n        'You are running ExLlamaV2 without flash-attention. This will cause the VRAM usage '\n        'to be a lot higher than it could be.\\n'\n        'Try installing flash-attention following the instructions here: '\n        'https://github.com/Dao-AILab/flash-attention#installation-and-features'\n    )\n    pass\nexcept Exception:\n    logger.warning('Failed to load flash-attention due to the following error:\\n')\n    traceback.print_exc()\n\n\nclass Exllamav2Model:\n    def __init__(self):\n        pass\n\n    @classmethod\n    def from_pretrained(self, path_to_model):\n\n        path_to_model = Path(f'{shared.args.model_dir}') / Path(path_to_model)\n\n        config = ExLlamaV2Config()\n        config.model_dir = str(path_to_model)\n        config.prepare()\n\n        config.max_seq_len = shared.args.max_seq_len\n        config.scale_pos_emb = shared.args.compress_pos_emb\n        config.scale_alpha_value = shared.args.alpha_value\n        config.no_flash_attn = shared.args.no_flash_attn\n        config.no_xformers = shared.args.no_xformers\n        config.no_sdpa = shared.args.no_sdpa\n        config.num_experts_per_token = int(shared.args.num_experts_per_token)\n\n        model = ExLlamaV2(config)\n\n        # Check if TP is enabled and load model with TP\n        if shared.args.enable_tp:\n            split = None\n            if shared.args.gpu_split:\n                split = [float(alloc) for alloc in shared.args.gpu_split.split(\",\")]\n            model.load_tp(split)  # Ensure TP loading is used\n        else:\n            if not shared.args.autosplit:\n                split = None\n                if shared.args.gpu_split:\n                    split = [float(alloc) for alloc in shared.args.gpu_split.split(\",\")]\n                model.load(split)\n\n        # Determine the correct cache type\n        if shared.args.cache_8bit:\n            cache_type = ExLlamaV2Cache_8bit\n        elif shared.args.cache_4bit:\n            cache_type = ExLlamaV2Cache_Q4\n        else:\n            cache_type = ExLlamaV2Cache\n\n        # Use TP if specified\n        if shared.args.enable_tp:\n            cache = ExLlamaV2Cache_TP(model, base=cache_type)\n        else:\n            cache = cache_type(model, lazy=shared.args.autosplit)\n\n        if shared.args.autosplit and not shared.args.enable_tp:\n            model.load_autosplit(cache)\n\n        tokenizer = ExLlamaV2Tokenizer(config)\n        generator = ExLlamaV2StreamingGenerator(model, cache, tokenizer)\n\n        result = self()\n        result.model = model\n        result.cache = cache\n        result.tokenizer = tokenizer\n        result.generator = generator\n        result.loras = None\n        return result, result\n\n    def encode(self, string, **kwargs):\n        return self.tokenizer.encode(string, add_bos=True, encode_special_tokens=True)\n\n    def decode(self, ids, **kwargs):\n        if isinstance(ids, list):\n            ids = torch.tensor([ids])\n        elif isinstance(ids, torch.Tensor) and ids.numel() == 1:\n            ids = ids.view(1, -1)\n\n        return self.tokenizer.decode(ids, decode_special_tokens=True)[0]\n\n    def get_logits(self, token_ids, **kwargs):\n        self.cache.current_seq_len = 0\n        if token_ids.shape[-1] > 1:\n            self.model.forward(token_ids[:, :-1], self.cache, input_mask=None, preprocess_only=True, loras=self.loras)\n\n        return self.model.forward(token_ids[:, -1:], self.cache, input_mask=None, loras=self.loras, **kwargs).float().cpu()\n\n    def generate_with_streaming(self, prompt, state):\n        settings = ExLlamaV2Sampler.Settings()\n\n        settings.token_repetition_penalty = state['repetition_penalty']\n        settings.token_repetition_range = -1 if state['repetition_penalty_range'] <= 0 else state['repetition_penalty_range']\n\n        settings.token_frequency_penalty = state['frequency_penalty']\n        settings.token_presence_penalty = state['presence_penalty']\n\n        settings.temperature = state['temperature']\n        settings.top_k = state['top_k']\n        settings.top_p = state['top_p']\n        settings.top_a = state['top_a']\n        settings.min_p = state['min_p']\n        settings.tfs = state['tfs']\n        settings.typical = state['typical_p']\n\n        settings.temperature_last = state['temperature_last']\n\n        settings.mirostat = state['mirostat_mode'] == 2\n        settings.mirostat_tau = state['mirostat_tau']\n        settings.mirostat_eta = state['mirostat_eta']\n\n        if state['ban_eos_token']:\n            settings.disallow_tokens(self.tokenizer, [self.tokenizer.eos_token_id])\n\n        if state['custom_token_bans']:\n            to_ban = [int(x) for x in state['custom_token_bans'].split(',')]\n            if len(to_ban) > 0:\n                settings.disallow_tokens(self.tokenizer, to_ban)\n\n        ids = self.tokenizer.encode(prompt, add_bos=state['add_bos_token'], encode_special_tokens=True)\n        ids = ids[:, -get_max_prompt_length(state):]\n\n        if state['auto_max_new_tokens']:\n            max_new_tokens = state['truncation_length'] - ids.shape[-1]\n        else:\n            max_new_tokens = state['max_new_tokens']\n\n        self.generator.begin_stream(ids, settings, loras=self.loras)\n\n        decoded_text = ''\n        for i in range(max_new_tokens):\n            chunk, eos, _ = self.generator.stream()\n            if eos or shared.stop_everything:\n                break\n\n            decoded_text += chunk\n            yield decoded_text\n\n    def generate(self, prompt, state):\n        output = ''\n        for output in self.generate_with_streaming(prompt, state):\n            pass\n\n        return output\n\n\nExllamav2_HF loader I want to use tensor parallelism with:\n\nimport os\nimport traceback\nfrom pathlib import Path\nfrom typing import Any, Dict, Optional, Union\n\nimport torch\nfrom exllamav2 import (\n    ExLlamaV2,\n    ExLlamaV2Cache,\n    ExLlamaV2Cache_8bit,\n    ExLlamaV2Cache_Q4,\n    ExLlamaV2Config\n)\nfrom torch.nn import CrossEntropyLoss\nfrom transformers import GenerationConfig, PretrainedConfig, PreTrainedModel\nfrom transformers.modeling_outputs import CausalLMOutputWithPast\n\nfrom modules import shared\nfrom modules.logging_colors import logger\n\ntry:\n    import flash_attn\nexcept ModuleNotFoundError:\n    logger.warning(\n        'You are running ExLlamaV2 without flash-attention. This will cause the VRAM usage '\n        'to be a lot higher than it could be.\\n'\n        'Try installing flash-attention following the instructions here: '\n        'https://github.com/Dao-AILab/flash-attention#installation-and-features'\n    )\n    pass\nexcept Exception:\n    logger.warning('Failed to load flash-attention due to the following error:\\n')\n    traceback.print_exc()\n\n\nclass Exllamav2HF(PreTrainedModel):\n    def __init__(self, config: ExLlamaV2Config):\n        super().__init__(PretrainedConfig())\n        self.ex_config = config\n        self.loras = None\n        self.generation_config = GenerationConfig()\n\n        self.ex_model = ExLlamaV2(config)\n\n        if not shared.args.autosplit:\n            split = None\n            if shared.args.gpu_split:\n                split = [float(alloc) for alloc in shared.args.gpu_split.split(\",\")]\n\n            self.ex_model.load(split)\n\n        if shared.args.cache_8bit:\n            self.ex_cache = ExLlamaV2Cache_8bit(self.ex_model, lazy=shared.args.autosplit)\n        elif shared.args.cache_4bit:\n            self.ex_cache = ExLlamaV2Cache_Q4(self.ex_model, lazy=shared.args.autosplit)\n        else:\n            self.ex_cache = ExLlamaV2Cache(self.ex_model, lazy=shared.args.autosplit)\n\n        if shared.args.autosplit:\n            self.ex_model.load_autosplit(self.ex_cache)\n\n        self.past_seq = None\n        if shared.args.cfg_cache:\n            if shared.args.cache_8bit:\n                self.ex_cache_negative = ExLlamaV2Cache_8bit(self.ex_model)\n            elif shared.args.cache_4bit:\n                self.ex_cache_negative = ExLlamaV2Cache_Q4(self.ex_model)\n            else:\n                self.ex_cache_negative = ExLlamaV2Cache(self.ex_model)\n\n            self.past_seq_negative = None\n\n    def _validate_model_class(self):\n        pass\n\n    def _validate_model_kwargs(self, model_kwargs: Dict[str, Any]):\n        pass\n\n    def prepare_inputs_for_generation(self, input_ids, **kwargs):\n        return {'input_ids': input_ids, **kwargs}\n\n    @property\n    def device(self) -> torch.device:\n        return torch.device(0)\n\n    def __call__(self, *args, **kwargs):\n        use_cache = kwargs.get('use_cache', True)\n        labels = kwargs.get('labels', None)\n        past_key_values = kwargs.get('past_key_values', None)\n\n        if len(args) > 0:\n            if not shared.args.cfg_cache:\n                logger.error(\"Please enable the cfg-cache option to use CFG with ExLlamav2_HF.\")\n                return\n\n            input_ids = args[0]\n            is_negative = True\n            past_seq = self.past_seq_negative\n            ex_cache = self.ex_cache_negative\n        else:\n            input_ids = kwargs['input_ids']\n            is_negative = False\n            past_seq = self.past_seq\n            ex_cache = self.ex_cache\n\n        seq = input_ids[0].tolist()\n        if is_negative and past_key_values is not None:\n            seq = past_key_values + seq\n\n        seq_tensor = torch.tensor(seq)\n        reset = True\n\n        # Make the forward call\n        if labels is None:\n            if past_seq is not None:\n                min_length = min(past_seq.shape[0], seq_tensor.shape[0])\n                indices = torch.nonzero(~torch.eq(past_seq[:min_length], seq_tensor[:min_length]))\n                if len(indices) > 0:\n                    longest_prefix = indices[0].item()\n                else:\n                    longest_prefix = min_length\n\n                if longest_prefix > 0:\n                    reset = False\n                    ex_cache.current_seq_len = longest_prefix\n                    if len(seq_tensor) - longest_prefix > 1:\n                        self.ex_model.forward(seq_tensor[longest_prefix:-1].view(1, -1), ex_cache, preprocess_only=True, loras=self.loras)\n                    elif len(seq_tensor) == longest_prefix:\n                        # Very tricky: if the prefix we are reusing *is* the input_ids, then we have to back up the cache pointer by one,\n                        # because we feed input_ids[-1] to forward() below, but that last token is already in the cache!\n                        ex_cache.current_seq_len -= 1\n\n            if reset:\n                ex_cache.current_seq_len = 0\n                if len(seq_tensor) > 1:\n                    self.ex_model.forward(seq_tensor[:-1].view(1, -1), ex_cache, preprocess_only=True, loras=self.loras)\n\n            logits = self.ex_model.forward(seq_tensor[-1:].view(1, -1), ex_cache, loras=self.loras).to(input_ids.device).float()\n        else:\n            ex_cache.current_seq_len = 0\n            logits = self.ex_model.forward(seq_tensor.view(1, -1), ex_cache, last_id_only=False, loras=self.loras).float()\n\n        if is_negative:\n            self.past_seq_negative = seq_tensor\n        else:\n            self.past_seq = seq_tensor\n\n        loss = None\n        if labels is not None:\n            # Shift so that tokens < n predict n\n            shift_logits = logits[..., :-1, :].contiguous()\n            shift_labels = labels[..., 1:].contiguous()\n            # Flatten the tokens\n            loss_fct = CrossEntropyLoss()\n            shift_logits = shift_logits.view(-1, logits.shape[-1])\n            shift_labels = shift_labels.view(-1)\n            # Enable model parallelism\n            shift_labels = shift_labels.to(shift_logits.device)\n            loss = loss_fct(shift_logits, shift_labels)\n\n        return CausalLMOutputWithPast(logits=logits, past_key_values=seq if use_cache else None, loss=loss)\n\n    @classmethod\n    def from_pretrained(cls, pretrained_model_name_or_path: Optional[Union[str, os.PathLike]], *model_args, **kwargs):\n        assert len(model_args) == 0 and len(kwargs) == 0, \"extra args is currently not supported\"\n        if isinstance(pretrained_model_name_or_path, str):\n            pretrained_model_name_or_path = Path(pretrained_model_name_or_path)\n\n        pretrained_model_name_or_path = Path(f'{shared.args.model_dir}') / Path(pretrained_model_name_or_path)\n\n        config = ExLlamaV2Config()\n        config.model_dir = str(pretrained_model_name_or_path)\n        config.prepare()\n\n        config.max_seq_len = shared.args.max_seq_len\n        config.scale_pos_emb = shared.args.compress_pos_emb\n        config.scale_alpha_value = shared.args.alpha_value\n        config.no_flash_attn = shared.args.no_flash_attn\n        config.no_xformers = shared.args.no_xformers\n        config.no_sdpa = shared.args.no_sdpa\n        config.num_experts_per_token = int(shared.args.num_experts_per_token)\n\n        return Exllamav2HF(config)",
            "To integrate tensor parallelism (TP) into the `Exllamav2HF` loader, you'll need to make a few modifications similar to what you have in the `Exllamav2Model` class. Specifically, you'll need to add the logic to handle TP when loading the model and adjust the cache initialization accordingly.\n\nHere's how you can modify the `Exllamav2HF` class to include tensor parallelism:\n\n```python\nimport os\nimport traceback\nfrom pathlib import Path\nfrom typing import Any, Dict, Optional, Union\n\nimport torch\nfrom exllamav2 import (\n    ExLlamaV2,\n    ExLlamaV2Cache,\n    ExLlamaV2Cache_8bit,\n    ExLlamaV2Cache_Q4,\n    ExLlamaV2Config,\n    ExLlamaV2Cache_TP,\n)\nfrom torch.nn import CrossEntropyLoss\nfrom transformers import GenerationConfig, PretrainedConfig, PreTrainedModel\nfrom transformers.modeling_outputs import CausalLMOutputWithPast\n\nfrom modules import shared\nfrom modules.logging_colors import logger\n\ntry:\n    import flash_attn\nexcept ModuleNotFoundError:\n    logger.warning(\n        'You are running ExLlamaV2 without flash-attention. This will cause the VRAM usage '\n        'to be a lot higher than it could be.\\n'\n        'Try installing flash-attention following the instructions here: '\n        'https://github.com/Dao-AILab/flash-attention#installation-and-features'\n    )\n    pass\nexcept Exception:\n    logger.warning('Failed to load flash-attention due to the following error:\\n')\n    traceback.print_exc()\n\nclass Exllamav2HF(PreTrainedModel):\n    def __init__(self, config: ExLlamaV2Config):\n        super().__init__(PretrainedConfig())\n        self.ex_config = config\n        self.loras = None\n        self.generation_config = GenerationConfig()\n\n        self.ex_model = ExLlamaV2(config)\n\n        # Check if TP is enabled and load model with TP\n        if shared.args.enable_tp:\n            split = None\n            if shared.args.gpu_split:\n                split = [float(alloc) for alloc in shared.args.gpu_split.split(\",\")]\n            self.ex_model.load_tp(split)  # Ensure TP loading is used\n        else:\n            if not shared.args.autosplit:\n                split = None\n                if shared.args.gpu_split:\n                    split = [float(alloc) for alloc in shared.args.gpu_split.split(\",\")]\n                self.ex_model.load(split)\n\n        # Determine the correct cache type\n        if shared.args.cache_8bit:\n            cache_type = ExLlamaV2Cache_8bit\n        elif shared.args.cache_4bit:\n            cache_type = ExLlamaV2Cache_Q4\n        else:\n            cache_type = ExLlamaV2Cache\n\n        # Use TP if specified\n        if shared.args.enable_tp:\n            self.ex_cache = ExLlamaV2Cache_TP(self.ex_model, base=cache_type)\n        else:\n            self.ex_cache = cache_type(self.ex_model, lazy=shared.args.autosplit)\n\n        if shared.args.autosplit and not shared.args.enable_tp:\n            self.ex_model.load_autosplit(self.ex_cache)\n\n        self.past_seq = None\n        if shared.args.cfg_cache:\n            if shared.args.cache_8bit:\n                self.ex_cache_negative = ExLlamaV2Cache_8bit(self.ex_model)\n            elif shared.args.cache_4bit:\n                self.ex_cache_negative = ExLlamaV2Cache_Q4(self.ex_model)\n            else:\n                self.ex_cache_negative = ExLlamaV2Cache(self.ex_model)\n\n            self.past_seq_negative = None\n\n    def _validate_model_class(self):\n        pass\n\n    def _validate_model_kwargs(self, model_kwargs: Dict[str, Any]):\n        pass\n\n    def prepare_inputs_for_generation(self, input_ids, **kwargs):\n        return {'input_ids': input_ids, **kwargs}\n\n    @property\n    def device(self) -> torch.device:\n        return torch.device(0)\n\n    def __call__(self, *args, **kwargs):\n        use_cache = kwargs.get('use_cache', True)\n        labels = kwargs.get('labels', None)\n        past_key_values = kwargs.get('past_key_values', None)\n\n        if len(args) > 0:\n            if not shared.args.cfg_cache:\n                logger.error(\"Please enable the cfg-cache option to use CFG with ExLlamav2_HF.\")\n                return\n\n            input_ids = args[0]\n            is_negative = True\n            past_seq = self.past_seq_negative\n            ex_cache = self.ex_cache_negative\n        else:\n            input_ids = kwargs['input_ids']\n            is_negative = False\n            past_seq = self.past_seq\n            ex_cache = self.ex_cache\n\n        seq = input_ids[0].tolist()\n        if is_negative and past_key_values is not None:\n            seq = past_key_values + seq\n\n        seq_tensor = torch.tensor(seq)\n        reset = True\n\n        # Make the forward call\n        if labels is None:\n            if past_seq is not None:\n                min_length = min(past_seq.shape[0], seq_tensor.shape[0])\n                indices = torch.nonzero(~torch.eq(past_seq[:min_length], seq_tensor[:min_length]))\n                if len(indices) > 0:\n                    longest_prefix = indices[0].item()\n                else:\n                    longest_prefix = min_length\n\n                if longest_prefix > 0:\n                    reset = False\n                    ex_cache.current_seq_len = longest_prefix\n                    if len(seq_tensor) - longest_prefix > 1:\n                        self.ex_model.forward(seq_tensor[longest_prefix:-1].view(1, -1), ex_cache, preprocess_only=True, loras=self.loras)\n                    elif len(seq_tensor) == longest_prefix:\n                        # Very tricky: if the prefix we are reusing *is* the input_ids, then we have to back up the cache pointer by one,\n                        # because we feed input_ids[-1] to forward() below, but that last token is already in the cache!\n                        ex_cache.current_seq_len -= 1\n\n            if reset:\n                ex_cache.current_seq_len = 0\n                if len(seq_tensor) > 1:\n                    self.ex_model.forward(seq_tensor[:-1].view(1, -1), ex_cache, preprocess_only=True, loras=self.loras)\n\n            logits = self.ex_model.forward(seq_tensor[-1:].view(1, -1), ex_cache, loras=self.loras).to(input_ids.device).float()\n        else:\n            ex_cache.current_seq_len = 0\n            logits = self.ex_model.forward(seq_tensor.view(1, -1), ex_cache, last_id_only=False, loras=self.loras).float()\n\n        if is_negative:\n            self.past_seq_negative = seq_tensor\n        else:\n            self.past_seq = seq_tensor\n\n        loss = None\n        if labels is not None:\n            # Shift so that tokens < n predict n\n            shift_logits = logits[..., :-1, :].contiguous()\n            shift_labels = labels[..., 1:].contiguous()\n            # Flatten the tokens\n            loss_fct = CrossEntropyLoss()\n            shift_logits = shift_logits.view(-1, logits.shape[-1])\n            shift_labels = shift_labels.view(-1)\n            # Enable model parallelism\n            shift_labels = shift_labels.to(shift_logits.device)\n            loss = loss_fct(shift_logits, shift_labels)\n\n        return CausalLMOutputWithPast(logits=logits, past_key_values=seq if use_cache else None, loss=loss)\n\n    @classmethod\n    def from_pretrained(cls, pretrained_model_name_or_path: Optional[Union[str, os.PathLike]], *model_args, **kwargs):\n        assert len(model_args) == 0 and len(kwargs) == 0, \"extra args is currently not supported\"\n        if isinstance(pretrained_model_name_or_path, str):\n            pretrained_model_name_or_path = Path(pretrained_model_name_or_path)\n\n        pretrained_model_name_or_path = Path(f'{shared.args.model_dir}') / Path(pretrained_model_name_or_path)\n\n        config = ExLlamaV2Config()\n        config.model_dir = str(pretrained_model_name_or_path)\n        config.prepare()\n\n        config.max_seq_len = shared.args.max_seq_len\n        config.scale_pos_emb = shared.args.compress_pos_emb\n        config.scale_alpha_value = shared.args.alpha_value\n        config.no_flash_attn = shared.args.no_flash_attn\n        config.no_xformers = shared.args.no_xformers\n        config.no_sdpa = shared.args.no_sdpa\n        config.num_experts_per_token = int(shared.args.num_experts_per_token)\n\n        return Exllamav2HF(config)\n```\n\nIn this modified version, I've added the logic to check if tensor parallelism (TP) is enabled and to load the model with TP if it is. I've also adjusted the cache initialization to use `ExLlamaV2Cache_TP` if TP is enabled. This should allow you to use tensor parallelism with the `Exllamav2HF` loader."
        ]
    ],
    "visible": [
        [
            "",
            "How can I help you today?"
        ],
        [
            "Hello, how are you functioning today?",
            "I&#x27;m functioning well, thank you for asking! I&#x27;m here and ready to assist you. How about you? How&#x27;s your day going?"
        ],
        [
            "I&#x27;m doing well too.  There is this quantization for LLMs called exllamaV2, recently it has implemented tensor parallelism, so folks running models on multi-gpu systems can inference faster....much faster in fact.  ExllamaV2 is a loader in a framework called textgeneration webui.  This allows different loaders such as ExllamaV2.\n\nSo I have exllamaV2 working with textgen using the exllamav2 loader, but i want to also get it working with the exllamav2_HF loader, this is a huggingface specific implementation of exllamav2.  I&#x27;ll give you the code for both:\n\nCurrently working exllamav2 code:\n #!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n&quot;&quot;&quot;\nCreated on Thu Aug 29 18:06:29 2024\n\n@author: myself\n&quot;&quot;&quot;\n\nimport traceback\nfrom pathlib import Path\n\nimport torch\nfrom exllamav2 import (\n      ExLlamaV2,\n    ExLlamaV2Config,\n    ExLlamaV2Cache,\n    ExLlamaV2Cache_8bit,\n    ExLlamaV2Cache_Q4,\n    ExLlamaV2Cache_Q6,\n    ExLlamaV2Cache_Q8,\n    ExLlamaV2Cache_TP,\n    ExLlamaV2Tokenizer,\n    model_init,\n)\nfrom exllamav2.generator import ExLlamaV2Sampler, ExLlamaV2StreamingGenerator\n\nfrom modules import shared\nfrom modules.logging_colors import logger\nfrom modules.text_generation import get_max_prompt_length\n\ntry:\n    import flash_attn\nexcept ModuleNotFoundError:\n    logger.warning(\n        &#x27;You are running ExLlamaV2 without flash-attention. This will cause the VRAM usage &#x27;\n        &#x27;to be a lot higher than it could be.\\n&#x27;\n        &#x27;Try installing flash-attention following the instructions here: &#x27;\n        &#x27;https://github.com/Dao-AILab/flash-attention#installation-and-features&#x27;\n    )\n    pass\nexcept Exception:\n    logger.warning(&#x27;Failed to load flash-attention due to the following error:\\n&#x27;)\n    traceback.print_exc()\n\n\nclass Exllamav2Model:\n    def __init__(self):\n        pass\n\n    @classmethod\n    def from_pretrained(self, path_to_model):\n\n        path_to_model = Path(f&#x27;{shared.args.model_dir}&#x27;) / Path(path_to_model)\n\n        config = ExLlamaV2Config()\n        config.model_dir = str(path_to_model)\n        config.prepare()\n\n        config.max_seq_len = shared.args.max_seq_len\n        config.scale_pos_emb = shared.args.compress_pos_emb\n        config.scale_alpha_value = shared.args.alpha_value\n        config.no_flash_attn = shared.args.no_flash_attn\n        config.no_xformers = shared.args.no_xformers\n        config.no_sdpa = shared.args.no_sdpa\n        config.num_experts_per_token = int(shared.args.num_experts_per_token)\n\n        model = ExLlamaV2(config)\n\n        # Check if TP is enabled and load model with TP\n        if shared.args.enable_tp:\n            split = None\n            if shared.args.gpu_split:\n                split = [float(alloc) for alloc in shared.args.gpu_split.split(&quot;,&quot;)]\n            model.load_tp(split)  # Ensure TP loading is used\n        else:\n            if not shared.args.autosplit:\n                split = None\n                if shared.args.gpu_split:\n                    split = [float(alloc) for alloc in shared.args.gpu_split.split(&quot;,&quot;)]\n                model.load(split)\n\n        # Determine the correct cache type\n        if shared.args.cache_8bit:\n            cache_type = ExLlamaV2Cache_8bit\n        elif shared.args.cache_4bit:\n            cache_type = ExLlamaV2Cache_Q4\n        else:\n            cache_type = ExLlamaV2Cache\n\n        # Use TP if specified\n        if shared.args.enable_tp:\n            cache = ExLlamaV2Cache_TP(model, base=cache_type)\n        else:\n            cache = cache_type(model, lazy=shared.args.autosplit)\n\n        if shared.args.autosplit and not shared.args.enable_tp:\n            model.load_autosplit(cache)\n\n        tokenizer = ExLlamaV2Tokenizer(config)\n        generator = ExLlamaV2StreamingGenerator(model, cache, tokenizer)\n\n        result = self()\n        result.model = model\n        result.cache = cache\n        result.tokenizer = tokenizer\n        result.generator = generator\n        result.loras = None\n        return result, result\n\n    def encode(self, string, **kwargs):\n        return self.tokenizer.encode(string, add_bos=True, encode_special_tokens=True)\n\n    def decode(self, ids, **kwargs):\n        if isinstance(ids, list):\n            ids = torch.tensor([ids])\n        elif isinstance(ids, torch.Tensor) and ids.numel() == 1:\n            ids = ids.view(1, -1)\n\n        return self.tokenizer.decode(ids, decode_special_tokens=True)[0]\n\n    def get_logits(self, token_ids, **kwargs):\n        self.cache.current_seq_len = 0\n        if token_ids.shape[-1] &gt; 1:\n            self.model.forward(token_ids[:, :-1], self.cache, input_mask=None, preprocess_only=True, loras=self.loras)\n\n        return self.model.forward(token_ids[:, -1:], self.cache, input_mask=None, loras=self.loras, **kwargs).float().cpu()\n\n    def generate_with_streaming(self, prompt, state):\n        settings = ExLlamaV2Sampler.Settings()\n\n        settings.token_repetition_penalty = state[&#x27;repetition_penalty&#x27;]\n        settings.token_repetition_range = -1 if state[&#x27;repetition_penalty_range&#x27;] &lt;= 0 else state[&#x27;repetition_penalty_range&#x27;]\n\n        settings.token_frequency_penalty = state[&#x27;frequency_penalty&#x27;]\n        settings.token_presence_penalty = state[&#x27;presence_penalty&#x27;]\n\n        settings.temperature = state[&#x27;temperature&#x27;]\n        settings.top_k = state[&#x27;top_k&#x27;]\n        settings.top_p = state[&#x27;top_p&#x27;]\n        settings.top_a = state[&#x27;top_a&#x27;]\n        settings.min_p = state[&#x27;min_p&#x27;]\n        settings.tfs = state[&#x27;tfs&#x27;]\n        settings.typical = state[&#x27;typical_p&#x27;]\n\n        settings.temperature_last = state[&#x27;temperature_last&#x27;]\n\n        settings.mirostat = state[&#x27;mirostat_mode&#x27;] == 2\n        settings.mirostat_tau = state[&#x27;mirostat_tau&#x27;]\n        settings.mirostat_eta = state[&#x27;mirostat_eta&#x27;]\n\n        if state[&#x27;ban_eos_token&#x27;]:\n            settings.disallow_tokens(self.tokenizer, [self.tokenizer.eos_token_id])\n\n        if state[&#x27;custom_token_bans&#x27;]:\n            to_ban = [int(x) for x in state[&#x27;custom_token_bans&#x27;].split(&#x27;,&#x27;)]\n            if len(to_ban) &gt; 0:\n                settings.disallow_tokens(self.tokenizer, to_ban)\n\n        ids = self.tokenizer.encode(prompt, add_bos=state[&#x27;add_bos_token&#x27;], encode_special_tokens=True)\n        ids = ids[:, -get_max_prompt_length(state):]\n\n        if state[&#x27;auto_max_new_tokens&#x27;]:\n            max_new_tokens = state[&#x27;truncation_length&#x27;] - ids.shape[-1]\n        else:\n            max_new_tokens = state[&#x27;max_new_tokens&#x27;]\n\n        self.generator.begin_stream(ids, settings, loras=self.loras)\n\n        decoded_text = &#x27;&#x27;\n        for i in range(max_new_tokens):\n            chunk, eos, _ = self.generator.stream()\n            if eos or shared.stop_everything:\n                break\n\n            decoded_text += chunk\n            yield decoded_text\n\n    def generate(self, prompt, state):\n        output = &#x27;&#x27;\n        for output in self.generate_with_streaming(prompt, state):\n            pass\n\n        return output\n\n\nExllamav2_HF loader I want to use tensor parallelism with:\n\nimport os\nimport traceback\nfrom pathlib import Path\nfrom typing import Any, Dict, Optional, Union\n\nimport torch\nfrom exllamav2 import (\n    ExLlamaV2,\n    ExLlamaV2Cache,\n    ExLlamaV2Cache_8bit,\n    ExLlamaV2Cache_Q4,\n    ExLlamaV2Config\n)\nfrom torch.nn import CrossEntropyLoss\nfrom transformers import GenerationConfig, PretrainedConfig, PreTrainedModel\nfrom transformers.modeling_outputs import CausalLMOutputWithPast\n\nfrom modules import shared\nfrom modules.logging_colors import logger\n\ntry:\n    import flash_attn\nexcept ModuleNotFoundError:\n    logger.warning(\n        &#x27;You are running ExLlamaV2 without flash-attention. This will cause the VRAM usage &#x27;\n        &#x27;to be a lot higher than it could be.\\n&#x27;\n        &#x27;Try installing flash-attention following the instructions here: &#x27;\n        &#x27;https://github.com/Dao-AILab/flash-attention#installation-and-features&#x27;\n    )\n    pass\nexcept Exception:\n    logger.warning(&#x27;Failed to load flash-attention due to the following error:\\n&#x27;)\n    traceback.print_exc()\n\n\nclass Exllamav2HF(PreTrainedModel):\n    def __init__(self, config: ExLlamaV2Config):\n        super().__init__(PretrainedConfig())\n        self.ex_config = config\n        self.loras = None\n        self.generation_config = GenerationConfig()\n\n        self.ex_model = ExLlamaV2(config)\n\n        if not shared.args.autosplit:\n            split = None\n            if shared.args.gpu_split:\n                split = [float(alloc) for alloc in shared.args.gpu_split.split(&quot;,&quot;)]\n\n            self.ex_model.load(split)\n\n        if shared.args.cache_8bit:\n            self.ex_cache = ExLlamaV2Cache_8bit(self.ex_model, lazy=shared.args.autosplit)\n        elif shared.args.cache_4bit:\n            self.ex_cache = ExLlamaV2Cache_Q4(self.ex_model, lazy=shared.args.autosplit)\n        else:\n            self.ex_cache = ExLlamaV2Cache(self.ex_model, lazy=shared.args.autosplit)\n\n        if shared.args.autosplit:\n            self.ex_model.load_autosplit(self.ex_cache)\n\n        self.past_seq = None\n        if shared.args.cfg_cache:\n            if shared.args.cache_8bit:\n                self.ex_cache_negative = ExLlamaV2Cache_8bit(self.ex_model)\n            elif shared.args.cache_4bit:\n                self.ex_cache_negative = ExLlamaV2Cache_Q4(self.ex_model)\n            else:\n                self.ex_cache_negative = ExLlamaV2Cache(self.ex_model)\n\n            self.past_seq_negative = None\n\n    def _validate_model_class(self):\n        pass\n\n    def _validate_model_kwargs(self, model_kwargs: Dict[str, Any]):\n        pass\n\n    def prepare_inputs_for_generation(self, input_ids, **kwargs):\n        return {&#x27;input_ids&#x27;: input_ids, **kwargs}\n\n    @property\n    def device(self) -&gt; torch.device:\n        return torch.device(0)\n\n    def __call__(self, *args, **kwargs):\n        use_cache = kwargs.get(&#x27;use_cache&#x27;, True)\n        labels = kwargs.get(&#x27;labels&#x27;, None)\n        past_key_values = kwargs.get(&#x27;past_key_values&#x27;, None)\n\n        if len(args) &gt; 0:\n            if not shared.args.cfg_cache:\n                logger.error(&quot;Please enable the cfg-cache option to use CFG with ExLlamav2_HF.&quot;)\n                return\n\n            input_ids = args[0]\n            is_negative = True\n            past_seq = self.past_seq_negative\n            ex_cache = self.ex_cache_negative\n        else:\n            input_ids = kwargs[&#x27;input_ids&#x27;]\n            is_negative = False\n            past_seq = self.past_seq\n            ex_cache = self.ex_cache\n\n        seq = input_ids[0].tolist()\n        if is_negative and past_key_values is not None:\n            seq = past_key_values + seq\n\n        seq_tensor = torch.tensor(seq)\n        reset = True\n\n        # Make the forward call\n        if labels is None:\n            if past_seq is not None:\n                min_length = min(past_seq.shape[0], seq_tensor.shape[0])\n                indices = torch.nonzero(~torch.eq(past_seq[:min_length], seq_tensor[:min_length]))\n                if len(indices) &gt; 0:\n                    longest_prefix = indices[0].item()\n                else:\n                    longest_prefix = min_length\n\n                if longest_prefix &gt; 0:\n                    reset = False\n                    ex_cache.current_seq_len = longest_prefix\n                    if len(seq_tensor) - longest_prefix &gt; 1:\n                        self.ex_model.forward(seq_tensor[longest_prefix:-1].view(1, -1), ex_cache, preprocess_only=True, loras=self.loras)\n                    elif len(seq_tensor) == longest_prefix:\n                        # Very tricky: if the prefix we are reusing *is* the input_ids, then we have to back up the cache pointer by one,\n                        # because we feed input_ids[-1] to forward() below, but that last token is already in the cache!\n                        ex_cache.current_seq_len -= 1\n\n            if reset:\n                ex_cache.current_seq_len = 0\n                if len(seq_tensor) &gt; 1:\n                    self.ex_model.forward(seq_tensor[:-1].view(1, -1), ex_cache, preprocess_only=True, loras=self.loras)\n\n            logits = self.ex_model.forward(seq_tensor[-1:].view(1, -1), ex_cache, loras=self.loras).to(input_ids.device).float()\n        else:\n            ex_cache.current_seq_len = 0\n            logits = self.ex_model.forward(seq_tensor.view(1, -1), ex_cache, last_id_only=False, loras=self.loras).float()\n\n        if is_negative:\n            self.past_seq_negative = seq_tensor\n        else:\n            self.past_seq = seq_tensor\n\n        loss = None\n        if labels is not None:\n            # Shift so that tokens &lt; n predict n\n            shift_logits = logits[..., :-1, :].contiguous()\n            shift_labels = labels[..., 1:].contiguous()\n            # Flatten the tokens\n            loss_fct = CrossEntropyLoss()\n            shift_logits = shift_logits.view(-1, logits.shape[-1])\n            shift_labels = shift_labels.view(-1)\n            # Enable model parallelism\n            shift_labels = shift_labels.to(shift_logits.device)\n            loss = loss_fct(shift_logits, shift_labels)\n\n        return CausalLMOutputWithPast(logits=logits, past_key_values=seq if use_cache else None, loss=loss)\n\n    @classmethod\n    def from_pretrained(cls, pretrained_model_name_or_path: Optional[Union[str, os.PathLike]], *model_args, **kwargs):\n        assert len(model_args) == 0 and len(kwargs) == 0, &quot;extra args is currently not supported&quot;\n        if isinstance(pretrained_model_name_or_path, str):\n            pretrained_model_name_or_path = Path(pretrained_model_name_or_path)\n\n        pretrained_model_name_or_path = Path(f&#x27;{shared.args.model_dir}&#x27;) / Path(pretrained_model_name_or_path)\n\n        config = ExLlamaV2Config()\n        config.model_dir = str(pretrained_model_name_or_path)\n        config.prepare()\n\n        config.max_seq_len = shared.args.max_seq_len\n        config.scale_pos_emb = shared.args.compress_pos_emb\n        config.scale_alpha_value = shared.args.alpha_value\n        config.no_flash_attn = shared.args.no_flash_attn\n        config.no_xformers = shared.args.no_xformers\n        config.no_sdpa = shared.args.no_sdpa\n        config.num_experts_per_token = int(shared.args.num_experts_per_token)\n\n        return Exllamav2HF(config)",
            "To integrate tensor parallelism (TP) into the `Exllamav2HF` loader, you&#x27;ll need to make a few modifications similar to what you have in the `Exllamav2Model` class. Specifically, you&#x27;ll need to add the logic to handle TP when loading the model and adjust the cache initialization accordingly.\n\nHere&#x27;s how you can modify the `Exllamav2HF` class to include tensor parallelism:\n\n```python\nimport os\nimport traceback\nfrom pathlib import Path\nfrom typing import Any, Dict, Optional, Union\n\nimport torch\nfrom exllamav2 import (\n    ExLlamaV2,\n    ExLlamaV2Cache,\n    ExLlamaV2Cache_8bit,\n    ExLlamaV2Cache_Q4,\n    ExLlamaV2Config,\n    ExLlamaV2Cache_TP,\n)\nfrom torch.nn import CrossEntropyLoss\nfrom transformers import GenerationConfig, PretrainedConfig, PreTrainedModel\nfrom transformers.modeling_outputs import CausalLMOutputWithPast\n\nfrom modules import shared\nfrom modules.logging_colors import logger\n\ntry:\n    import flash_attn\nexcept ModuleNotFoundError:\n    logger.warning(\n        &#x27;You are running ExLlamaV2 without flash-attention. This will cause the VRAM usage &#x27;\n        &#x27;to be a lot higher than it could be.\\n&#x27;\n        &#x27;Try installing flash-attention following the instructions here: &#x27;\n        &#x27;https://github.com/Dao-AILab/flash-attention#installation-and-features&#x27;\n    )\n    pass\nexcept Exception:\n    logger.warning(&#x27;Failed to load flash-attention due to the following error:\\n&#x27;)\n    traceback.print_exc()\n\nclass Exllamav2HF(PreTrainedModel):\n    def __init__(self, config: ExLlamaV2Config):\n        super().__init__(PretrainedConfig())\n        self.ex_config = config\n        self.loras = None\n        self.generation_config = GenerationConfig()\n\n        self.ex_model = ExLlamaV2(config)\n\n        # Check if TP is enabled and load model with TP\n        if shared.args.enable_tp:\n            split = None\n            if shared.args.gpu_split:\n                split = [float(alloc) for alloc in shared.args.gpu_split.split(&quot;,&quot;)]\n            self.ex_model.load_tp(split)  # Ensure TP loading is used\n        else:\n            if not shared.args.autosplit:\n                split = None\n                if shared.args.gpu_split:\n                    split = [float(alloc) for alloc in shared.args.gpu_split.split(&quot;,&quot;)]\n                self.ex_model.load(split)\n\n        # Determine the correct cache type\n        if shared.args.cache_8bit:\n            cache_type = ExLlamaV2Cache_8bit\n        elif shared.args.cache_4bit:\n            cache_type = ExLlamaV2Cache_Q4\n        else:\n            cache_type = ExLlamaV2Cache\n\n        # Use TP if specified\n        if shared.args.enable_tp:\n            self.ex_cache = ExLlamaV2Cache_TP(self.ex_model, base=cache_type)\n        else:\n            self.ex_cache = cache_type(self.ex_model, lazy=shared.args.autosplit)\n\n        if shared.args.autosplit and not shared.args.enable_tp:\n            self.ex_model.load_autosplit(self.ex_cache)\n\n        self.past_seq = None\n        if shared.args.cfg_cache:\n            if shared.args.cache_8bit:\n                self.ex_cache_negative = ExLlamaV2Cache_8bit(self.ex_model)\n            elif shared.args.cache_4bit:\n                self.ex_cache_negative = ExLlamaV2Cache_Q4(self.ex_model)\n            else:\n                self.ex_cache_negative = ExLlamaV2Cache(self.ex_model)\n\n            self.past_seq_negative = None\n\n    def _validate_model_class(self):\n        pass\n\n    def _validate_model_kwargs(self, model_kwargs: Dict[str, Any]):\n        pass\n\n    def prepare_inputs_for_generation(self, input_ids, **kwargs):\n        return {&#x27;input_ids&#x27;: input_ids, **kwargs}\n\n    @property\n    def device(self) -&gt; torch.device:\n        return torch.device(0)\n\n    def __call__(self, *args, **kwargs):\n        use_cache = kwargs.get(&#x27;use_cache&#x27;, True)\n        labels = kwargs.get(&#x27;labels&#x27;, None)\n        past_key_values = kwargs.get(&#x27;past_key_values&#x27;, None)\n\n        if len(args) &gt; 0:\n            if not shared.args.cfg_cache:\n                logger.error(&quot;Please enable the cfg-cache option to use CFG with ExLlamav2_HF.&quot;)\n                return\n\n            input_ids = args[0]\n            is_negative = True\n            past_seq = self.past_seq_negative\n            ex_cache = self.ex_cache_negative\n        else:\n            input_ids = kwargs[&#x27;input_ids&#x27;]\n            is_negative = False\n            past_seq = self.past_seq\n            ex_cache = self.ex_cache\n\n        seq = input_ids[0].tolist()\n        if is_negative and past_key_values is not None:\n            seq = past_key_values + seq\n\n        seq_tensor = torch.tensor(seq)\n        reset = True\n\n        # Make the forward call\n        if labels is None:\n            if past_seq is not None:\n                min_length = min(past_seq.shape[0], seq_tensor.shape[0])\n                indices = torch.nonzero(~torch.eq(past_seq[:min_length], seq_tensor[:min_length]))\n                if len(indices) &gt; 0:\n                    longest_prefix = indices[0].item()\n                else:\n                    longest_prefix = min_length\n\n                if longest_prefix &gt; 0:\n                    reset = False\n                    ex_cache.current_seq_len = longest_prefix\n                    if len(seq_tensor) - longest_prefix &gt; 1:\n                        self.ex_model.forward(seq_tensor[longest_prefix:-1].view(1, -1), ex_cache, preprocess_only=True, loras=self.loras)\n                    elif len(seq_tensor) == longest_prefix:\n                        # Very tricky: if the prefix we are reusing *is* the input_ids, then we have to back up the cache pointer by one,\n                        # because we feed input_ids[-1] to forward() below, but that last token is already in the cache!\n                        ex_cache.current_seq_len -= 1\n\n            if reset:\n                ex_cache.current_seq_len = 0\n                if len(seq_tensor) &gt; 1:\n                    self.ex_model.forward(seq_tensor[:-1].view(1, -1), ex_cache, preprocess_only=True, loras=self.loras)\n\n            logits = self.ex_model.forward(seq_tensor[-1:].view(1, -1), ex_cache, loras=self.loras).to(input_ids.device).float()\n        else:\n            ex_cache.current_seq_len = 0\n            logits = self.ex_model.forward(seq_tensor.view(1, -1), ex_cache, last_id_only=False, loras=self.loras).float()\n\n        if is_negative:\n            self.past_seq_negative = seq_tensor\n        else:\n            self.past_seq = seq_tensor\n\n        loss = None\n        if labels is not None:\n            # Shift so that tokens &lt; n predict n\n            shift_logits = logits[..., :-1, :].contiguous()\n            shift_labels = labels[..., 1:].contiguous()\n            # Flatten the tokens\n            loss_fct = CrossEntropyLoss()\n            shift_logits = shift_logits.view(-1, logits.shape[-1])\n            shift_labels = shift_labels.view(-1)\n            # Enable model parallelism\n            shift_labels = shift_labels.to(shift_logits.device)\n            loss = loss_fct(shift_logits, shift_labels)\n\n        return CausalLMOutputWithPast(logits=logits, past_key_values=seq if use_cache else None, loss=loss)\n\n    @classmethod\n    def from_pretrained(cls, pretrained_model_name_or_path: Optional[Union[str, os.PathLike]], *model_args, **kwargs):\n        assert len(model_args) == 0 and len(kwargs) == 0, &quot;extra args is currently not supported&quot;\n        if isinstance(pretrained_model_name_or_path, str):\n            pretrained_model_name_or_path = Path(pretrained_model_name_or_path)\n\n        pretrained_model_name_or_path = Path(f&#x27;{shared.args.model_dir}&#x27;) / Path(pretrained_model_name_or_path)\n\n        config = ExLlamaV2Config()\n        config.model_dir = str(pretrained_model_name_or_path)\n        config.prepare()\n\n        config.max_seq_len = shared.args.max_seq_len\n        config.scale_pos_emb = shared.args.compress_pos_emb\n        config.scale_alpha_value = shared.args.alpha_value\n        config.no_flash_attn = shared.args.no_flash_attn\n        config.no_xformers = shared.args.no_xformers\n        config.no_sdpa = shared.args.no_sdpa\n        config.num_experts_per_token = int(shared.args.num_experts_per_token)\n\n        return Exllamav2HF(config)\n```\n\nIn this modified version, I&#x27;ve added the logic to check if tensor parallelism (TP) is enabled and to load the model with TP if it is. I&#x27;ve also adjusted the cache initialization to use `ExLlamaV2Cache_TP` if TP is enabled. This should allow you to use tensor parallelism with the `Exllamav2HF` loader."
        ]
    ]
}